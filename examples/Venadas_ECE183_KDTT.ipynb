{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"400\">](https://github.com/jeshraghian/snntorch/)\n",
        "\n",
        "# snnTorch - KL Divergence Theorem to Predict Signal Behaviors\n",
        "## A project on Knowledge Distillation\n",
        "### By Sean M. Venadas\n",
        "### With help from: Skye Gunasekaran, Jason Eshraghian"
      ],
      "metadata": {
        "id": "ueACyp3dPgkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This student-made tutorial was made for the class ECE183 (F2023) as part of UC Santa Cruz. It is only a small portion of a bigger project -- predicting\n",
        "seizures throught the use of SNNs on EEG signals. This subset project was\n",
        "designed to practice implementing KL Divergence and MSE to student-teacher models. We specifically used SNNs (compared to NNs) because of its temporal advantages and ability to compute data more quickly and accurately. If you have any questions, please consider emailing me: svenadas@ucsc.edu."
      ],
      "metadata": {
        "id": "suOzF8-kb8gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "YY6kbGrbdLW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, you will:\n",
        "* Generate a sinusoid signal\n",
        "* Create a teacher-model to train two different student-models\n",
        "* Implement MSE and KL Divergence to predict signal behaviors in the teacher-model\n",
        "* Plot data to visualize and compare results\n",
        "\n",
        "Install the required libraries by clicking into the following cell and pressing `Shift+Enter`."
      ],
      "metadata": {
        "id": "WfzgKR_zoOMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch"
      ],
      "metadata": {
        "id": "wFRyvTPadayG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "import statistics\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "wIXcaNH0dggX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Set Up the Signals"
      ],
      "metadata": {
        "id": "BJxGygA4fUKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get started, we need to set up two sinusoid signals to represent the teacher and student. Assuming basic knowledge on Knowledge Distillation (KD), you should know that you want your teacher model to be bigger in architecture. This is because we want to give the student as much information as possible."
      ],
      "metadata": {
        "id": "lREmepoyhJu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sinusoid_data(num_points, amplitude, frequency, phase, noise_std=0.1):\n",
        "    t = np.linspace(0, 1, num_points, dtype=np.float32)\n",
        "    sinusoid = amplitude * np.sin(2 * np.pi * frequency * t + phase)\n",
        "    noisy_sinusoid = sinusoid + np.random.normal(0, noise_std, size=num_points)\n",
        "    return t, noisy_sinusoid\n",
        "\n",
        "# The parameters below can be edited to your liking\n",
        "num_points = 100\n",
        "amplitude_teacher, frequency_teacher, phase_teacher = 1.0, 2.5, 0.0\n",
        "amplitude_student, frequency_student, phase_student = 1.0, 2.0, 0.5 #phase-shifted to test pattern detection"
      ],
      "metadata": {
        "id": "ZtdHacLVf6EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates signals with specified parameters in previous cell\n",
        "t_teacher, teacher_data = generate_sinusoid_data(num_points, amplitude_teacher, frequency_teacher, phase_teacher)\n",
        "t_student, student_data = generate_sinusoid_data(num_points, amplitude_student, frequency_student, phase_student)"
      ],
      "metadata": {
        "id": "aa6ZwpMKgEDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors with dtype float32\n",
        "t_teacher_tensor = torch.from_numpy(t_teacher).view(-1, 1).float()\n",
        "teacher_data_tensor = torch.from_numpy(teacher_data).view(-1, 1).float()\n",
        "\n",
        "t_student_tensor = torch.from_numpy(t_student).view(-1, 1).float()\n",
        "student_data_tensor = torch.from_numpy(student_data).view(-1, 1).float()"
      ],
      "metadata": {
        "id": "Rbo59IB1gHiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Define the Models"
      ],
      "metadata": {
        "id": "oBKsjzGkoTOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filler"
      ],
      "metadata": {
        "id": "Agy_sO2JoyIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Teacher Model"
      ],
      "metadata": {
        "id": "ANGaVQR2o2Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TeacherSinusoidModel(torch.nn.Module):\n",
        "    \"\"\"Simple spiking neural network in snntorch.\"\"\"\n",
        "\n",
        "    def __init__(self, timesteps=100, hidden=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.timesteps = timesteps # number of time steps to simulate the network\n",
        "        self.hidden = hidden # number of hidden neurons\n",
        "        spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
        "\n",
        "        # layer 1 (input layer)\n",
        "        beta1 = torch.rand(self.hidden)\n",
        "        thr1 = torch.rand(self.hidden)\n",
        "        self.fc1 = torch.nn.Linear(in_features=1, out_features=self.hidden)\n",
        "        self.lif1 = snn.Leaky(beta=beta1, threshold=thr1, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # layer 2 (hidden layer)\n",
        "        beta2 = torch.rand(self.hidden)\n",
        "        thr2 = torch.rand(self.hidden)\n",
        "        self.fc2 = torch.nn.Linear(in_features=self.hidden, out_features=self.hidden)\n",
        "        self.lif2 = snn.Leaky(beta=beta2, threshold=thr2, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # layer 3 (hidden layer)\n",
        "        beta3 = torch.rand(self.hidden)\n",
        "        thr3 = torch.rand(self.hidden)\n",
        "        self.fc3 = torch.nn.Linear(in_features=self.hidden, out_features=self.hidden)\n",
        "        self.lif3 = snn.Leaky(beta=beta3, threshold=thr3, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # layer 4 (hidden layer)\n",
        "        beta4 = torch.rand(self.hidden)\n",
        "        thr4 = torch.rand(self.hidden)\n",
        "        self.fc4 = torch.nn.Linear(in_features=self.hidden, out_features=self.hidden)\n",
        "        self.lif4 = snn.Leaky(beta=beta4, threshold=thr4, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # layer 5 (hidden layer)\n",
        "        beta5 = torch.rand(self.hidden)\n",
        "        thr5 = torch.rand(self.hidden)\n",
        "        self.fc5 = torch.nn.Linear(in_features=self.hidden, out_features=self.hidden)\n",
        "        self.lif5 = snn.Leaky(beta=beta5, threshold=thr5, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # layer 6 (output layer)\n",
        "        beta6 = torch.rand(1)\n",
        "        # layer 6: leaky integrator neuron. Note the reset mechanism is disabled and we will disregard output spikes.\n",
        "        self.fc6 = torch.nn.Linear(in_features=self.hidden, out_features=1)\n",
        "        self.lif6 = snn.Leaky(beta=beta6, threshold=1.0, learn_beta=True, spike_grad=spike_grad, reset_mechanism=\"none\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass for several time steps.\"\"\"\n",
        "\n",
        "        # Initalize membrane potential\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "        mem5 = self.lif5.init_leaky()\n",
        "        mem6 = self.lif6.init_leaky()\n",
        "\n",
        "        # Empty lists to record outputs\n",
        "        mem6_rec = []\n",
        "\n",
        "        # Loop over\n",
        "        for step in range(self.timesteps):\n",
        "            x_timestep = x[step, :]\n",
        "\n",
        "            cur1 = self.fc1(x_timestep)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "\n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            cur4 = self.fc4(spk3)\n",
        "            spk4, mem4 = self.lif4(cur4, mem4)\n",
        "\n",
        "            cur5 = self.fc5(spk4)\n",
        "            spk5, mem5 = self.lif5(cur5, mem5)\n",
        "\n",
        "            cur6 = self.fc6(spk5)\n",
        "            _, mem6 = self.lif6(cur6, mem6)\n",
        "\n",
        "            mem6_rec.append(mem6)\n",
        "\n",
        "        return torch.stack(mem6_rec)"
      ],
      "metadata": {
        "id": "2KTttDHKpEtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Student Model"
      ],
      "metadata": {
        "id": "UAAepG0Jo_-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentSinusoidModel(torch.nn.Module):\n",
        "    \"\"\"Simple spiking neural network in snntorch.\"\"\"\n",
        "\n",
        "    def __init__(self, timesteps=100, hidden=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.timesteps = timesteps # number of time steps to simulate the network\n",
        "        self.hidden = hidden # number of hidden neurons\n",
        "        spike_grad = surrogate.fast_sigmoid() # surrogate gradient function\n",
        "\n",
        "        # randomly initialize decay rate and threshold for layer 1\n",
        "        beta_in = torch.rand(self.hidden)\n",
        "        thr_in = torch.rand(self.hidden)\n",
        "\n",
        "        # layer 1\n",
        "        self.fc_in = torch.nn.Linear(in_features=1, out_features=self.hidden)\n",
        "        self.lif_in = snn.Leaky(beta=beta_in, threshold=thr_in, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # randomly initialize decay rate and threshold for layer 2\n",
        "        beta_hidden = torch.rand(self.hidden)\n",
        "        thr_hidden = torch.rand(self.hidden)\n",
        "\n",
        "        # layer 2\n",
        "        self.fc_hidden = torch.nn.Linear(in_features=self.hidden, out_features=self.hidden)\n",
        "        self.lif_hidden = snn.Leaky(beta=beta_hidden, threshold=thr_hidden, learn_beta=True, spike_grad=spike_grad)\n",
        "\n",
        "        # randomly initialize decay rate for output neuron\n",
        "        beta_out = torch.rand(1)\n",
        "\n",
        "        # layer 3: leaky integrator neuron. Note the reset mechanism is disabled and we will disregard output spikes.\n",
        "        self.fc_out = torch.nn.Linear(in_features=self.hidden, out_features=1)\n",
        "        self.li_out = snn.Leaky(beta=beta_out, threshold=1.0, learn_beta=True, spike_grad=spike_grad, reset_mechanism=\"none\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass for several time steps.\"\"\"\n",
        "\n",
        "        # Initalize membrane potential\n",
        "        mem_1 = self.lif_in.init_leaky()\n",
        "        mem_2 = self.lif_hidden.init_leaky()\n",
        "        mem_3 = self.li_out.init_leaky()\n",
        "\n",
        "        # Empty lists to record outputs\n",
        "        mem_3_rec = []\n",
        "\n",
        "        # Loop over\n",
        "        for step in range(self.timesteps):\n",
        "            x_timestep = x[step, :]\n",
        "\n",
        "            cur_in = self.fc_in(x_timestep)\n",
        "            spk_in, mem_1 = self.lif_in(cur_in, mem_1)\n",
        "\n",
        "            cur_hidden = self.fc_hidden(spk_in)\n",
        "            spk_hidden, mem_2 = self.lif_hidden(cur_hidden, mem_2)\n",
        "\n",
        "            cur_out = self.fc_out(spk_hidden)\n",
        "            _, mem_3 = self.li_out(cur_out, mem_3)\n",
        "\n",
        "            mem_3_rec.append(mem_3)\n",
        "\n",
        "        return torch.stack(mem_3_rec)"
      ],
      "metadata": {
        "id": "Uxt6vTyRpFjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implement Knowledge Distillation"
      ],
      "metadata": {
        "id": "d5-hTT1qqmF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filler"
      ],
      "metadata": {
        "id": "Gi7XQScgrDpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Evaluate the Models"
      ],
      "metadata": {
        "id": "KzietVForGI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filler"
      ],
      "metadata": {
        "id": "eOm6CwKWuNbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, input_data, target_data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input_data)\n",
        "        mse = nn.functional.mse_loss(predictions, target_data)\n",
        "    return mse.item()"
      ],
      "metadata": {
        "id": "vizZszvCuMmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Create the Teacher Model"
      ],
      "metadata": {
        "id": "wPPWUQWEsNAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create teacher model\n",
        "teacher_model = SinusoidModel()\n",
        "\n",
        "# Loss function without KL divergence term\n",
        "criterion_teacher = nn.MSELoss()\n",
        "\n",
        "# Create optimizer for teacher model\n",
        "teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=lr2)\n",
        "\n",
        "# Train the teacher model on teacher data\n",
        "for epoch in range(epochs2):\n",
        "    teacher_optimizer.zero_grad()\n",
        "    teacher_output = teacher_model(t_teacher_tensor)\n",
        "    loss_teacher = criterion_teacher(teacher_output, teacher_data_tensor)\n",
        "    loss_teacher.backward()\n",
        "    teacher_optimizer.step()\n",
        "\n",
        "# Get logits from the teacher model\n",
        "teacher_logits = teacher_model(t_teacher_tensor)"
      ],
      "metadata": {
        "id": "TUsYtRKWucFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Create the Student Models"
      ],
      "metadata": {
        "id": "izFkd15GsTIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filler"
      ],
      "metadata": {
        "id": "9N6Ye-4uuhDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the temporary student model\n",
        "\n",
        "# Create temporary student model\n",
        "temp_student_model = SinusoidModel()\n",
        "\n",
        "# Loss function without KL divergence term for the temporary student\n",
        "def custom_loss2(y_true, y_pred):\n",
        "    mse_loss = nn.functional.mse_loss(y_true, y_pred)\n",
        "    return mse_loss\n",
        "\n",
        "# Create optimizer for temporary student model\n",
        "temp_student_optimizer = optim.Adam(temp_student_model.parameters(), lr=lr)\n",
        "\n",
        "# Train the temporary student model on student data\n",
        "for epoch in range(epochs):\n",
        "    temp_student_optimizer.zero_grad()\n",
        "    temp_student_output = temp_student_model(t_student_tensor)\n",
        "    loss_temp_student = custom_loss2(student_data_tensor, temp_student_output)\n",
        "    loss_temp_student.backward()\n",
        "    temp_student_optimizer.step()"
      ],
      "metadata": {
        "id": "7b7CAboUuiz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the final student model\n",
        "\n",
        "# Create final student model\n",
        "student_model = SinusoidModel()\n",
        "\n",
        "# Loss function with KL divergence term\n",
        "def custom_loss1(y_true, y_pred, teacher_logits):\n",
        "    #mse_loss = nn.functional.mse_loss(y_true, y_pred)\n",
        "    mse_loss = nn.functional.mse_loss(y_true, y_pred)\n",
        "    kl_loss = nn.functional.kl_div(F.log_softmax(y_pred, dim=1), F.softmax(teacher_logits, dim=1), reduction='batchmean')\n",
        "    return mse_loss + kl_loss\n",
        "\n",
        "# Create optimizer for student model\n",
        "student_optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
        "\n",
        "# Train the student model on student data with KL divergence term\n",
        "for epoch in range(epochs):\n",
        "    teacher_model.eval()\n",
        "    student_optimizer.zero_grad()\n",
        "    student_output = student_model(t_student_tensor)\n",
        "    with torch.no_grad():\n",
        "      teacher_logits = teacher_model(t_student_tensor)\n",
        "    loss = custom_loss1(student_data_tensor, student_output, teacher_logits)\n",
        "    loss.backward(retain_graph=True)  # Set retain_graph=True to allow multiple backward passes\n",
        "    student_optimizer.step()"
      ],
      "metadata": {
        "id": "RolOwjlBuq-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Results"
      ],
      "metadata": {
        "id": "kISvE22UvDzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(t_teacher, teacher_data, label='Teacher Sinusoid')\n",
        "plt.plot(t_teacher, teacher_model(t_teacher_tensor).detach().numpy(), label='Teacher Model Prediction')\n",
        "plt.legend()\n",
        "plt.title('Teacher Model')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(t_student, student_data, label='Student Sinusoid')\n",
        "plt.plot(t_student, temp_student_model(t_student_tensor).detach().numpy(), label='Student Model Prediction')\n",
        "plt.legend()\n",
        "plt.title('Base Student Model')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(t_student, student_data, label='Student Sinusoid')\n",
        "plt.plot(t_student, student_model(t_student_tensor).detach().numpy(), label='Student Model Prediction (Temporal Distillation)')\n",
        "plt.legend()\n",
        "plt.title('Student Model (Temporal Distillation)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2MrlrTODvIKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Teacher Model\n",
        "mse_teacher = evaluate_model(teacher_model, t_teacher_tensor, teacher_data_tensor)\n",
        "print(f\"MSE for Teacher Model: {mse_teacher}\")\n",
        "\n",
        "# Evaluate Base Student Model\n",
        "mse_temp_student = evaluate_model(temp_student_model, t_student_tensor, student_data_tensor)\n",
        "print(f\"MSE for Base Student Model: {mse_temp_student}\")\n",
        "\n",
        "# Evaluate Student Model with Temporal Distillation\n",
        "mse_student = evaluate_model(student_model, t_student_tensor, student_data_tensor)\n",
        "print(f\"MSE for Student Model with Temporal Distillation: {mse_student}\")"
      ],
      "metadata": {
        "id": "TDEI7p79vO1R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}