{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d5313e-c29d-4581-a9c7-a45122337069",
   "metadata": {
    "id": "47d5313e-c29d-4581-a9c7-a45122337069"
   },
   "source": [
    "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"300\">](https://github.com/jeshraghian/snntorch/)\n",
    "\n",
    "# Discover SNN Hyperparameters with Optuna\n",
    "### Tutorial written by Reto Stamm\n",
    "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_optuna.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub-Mark-Light-120px-plus.png?raw=true' width=\"28\">](https://github.com/jeshraghian/snntorch/) [<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub_Logo_White.png?raw=true' width=\"80\">](https://github.com/jeshraghian/snntorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oll2NNFeG1NG",
   "metadata": {
    "id": "oll2NNFeG1NG"
   },
   "source": [
    "*This tutorial demonstrates optimizing Spiking Neural Network hyperparameters with Optuna, blending advanced neural modeling and hyperparameter tuning. In this example, we minimize power consumption by adjusting hyperparameter.*\n",
    "\n",
    "**[Optuna](https://optuna.org)** is an efficient, open-source hyperparameter optimization framework, streamlining ML model tuning with support for various algorithms and easy integration with key ML libraries. In other words, a tool that helps automatically figure out the best settings for machine learning models, making them perform better.\n",
    "\n",
    "**Spiked Neural Networks** (SNNs) model neural processing realistically, efficiently handling time-dependent data with low power. Their event-driven nature excels in dynamic, real-time tasks, advancing neuromorphic computing and AI. For an in-depth understanding of SNNs and their inner workings, check out the [snnTorch tutorial series](https://snntorch.readthedocs.io/en/latest/tutorials/index.html). Please cite the accompanying paper if you find these tutorials or code beneficial.\n",
    "\n",
    "For this example, let's assume that the more the neural network spikes, the more power it consumes. We want to **minimize power consumption**, so we adjust its **hyperparameters**, including the shape of the network, the number of steps run, and the number of epochs it is trained for.\n",
    "\n",
    "<cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". Proceedings of the IEEE, 111(9) September 2023.](https://ieeexplore.ieee.org/abstract/document/10242251) </cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hDnIEHOKB8LD",
   "metadata": {
    "id": "hDnIEHOKB8LD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install optuna snntorch torchvision matplotlib optunacy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WL487gZW1Agy",
   "metadata": {
    "id": "WL487gZW1Agy"
   },
   "outputs": [],
   "source": [
    "# Import all the libraries\n",
    "import copy\n",
    "import logging\n",
    "import random\n",
    "import numbers\n",
    "import sys\n",
    "import time # To see how long each iteration takes\n",
    "import multiprocessing # To check how many cores we have\n",
    "\n",
    "import optuna # the optimizer\n",
    "# To abort, or prune inefficient parameter sets\n",
    "from optuna.exceptions import TrialPruned\n",
    "from concurrent.futures import ThreadPoolExecutor # todo: concurrent execution\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Basic torch tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Image processing tools\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Extra plotting tools\n",
    "from optunacy.oplot import OPlot\n",
    "import scipy as scipy\n",
    "\n",
    "# Spiked Neural Networks!\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "from snntorch import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EYf13Gtx1OCj",
   "metadata": {
    "id": "EYf13Gtx1OCj"
   },
   "source": [
    "## 1. The MNIST Dataset\n",
    "### 1.1 Dataloading\n",
    "Define variables for dataloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eo4T5MC21hgD",
   "metadata": {
    "id": "eo4T5MC21hgD"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "data_path='/tmp/data/mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "myFKqNx11qYS",
   "metadata": {
    "id": "myFKqNx11qYS"
   },
   "source": [
    "Load the dataset.\n",
    "\n",
    "We don't use the test set for tuning hyperparameters to avoid losing the ability to generalize and to spot overfitting.\n",
    "Instead, we separate a validation set for this purpose.\n",
    "\n",
    "This should be the same subset each time, otherwise we \"learn\" the hyperparameters that are good for that set too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3GdglZjK04cb",
   "metadata": {
    "id": "3GdglZjK04cb"
   },
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "# The MNIST dataset contains black and white images (28x28) of digits from 0-9\n",
    "# 60,000 training images\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True,\n",
    "                             transform=transform)\n",
    "\n",
    "# Split out a validation subset\n",
    "total_size = len(mnist_train)\n",
    "val_size = int(total_size * 0.08)  # 8% for validation\n",
    "train_size = total_size - val_size  # Remaining for training\n",
    "\n",
    "# Split the dataset, the same way every time\n",
    "mnist_val = Subset(mnist_train, range(train_size, total_size))\n",
    "mnist_train = Subset(mnist_train, range(0, train_size))\n",
    "\n",
    "# 10,000 test images\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True,\n",
    "                            transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(mnist_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971046c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4971046c",
    "outputId": "65da68d3-09ea-41e1-b9d7-69ca5ab9347d"
   },
   "outputs": [],
   "source": [
    "# See what acceleration hardware we have available on this machine\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16306f",
   "metadata": {
    "id": "4c16306f"
   },
   "outputs": [],
   "source": [
    "# We need to use the logger so that the messages are in sync with Optunas output\n",
    "logger = logging.getLogger('optuna')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cGmB_RhWIklU",
   "metadata": {
    "id": "cGmB_RhWIklU"
   },
   "source": [
    "### 1.2 A description of the data\n",
    "MNIST is a collection of 70,000 images of handwritten numbers. It includes 60,000 training images and 10,000 test images. The images are simple black and white, showing digits from 0 to 9, and the aim is to correctly identify these digits.\n",
    "\n",
    "Key points about these images:\n",
    "\n",
    "* Each image is 28 pixels wide and 28 pixels tall.\n",
    "* They are in black and white, which means each pixel is just a shade of gray, not color.\n",
    "* These images don't change over time; they're just single, static pictures.\n",
    "\n",
    "It's good to look at the shape of the datastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4994fa9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4994fa9",
    "outputId": "c7090b52-01aa-4b61-c24d-f47813be76fd"
   },
   "outputs": [],
   "source": [
    "for data, label in iter(train_loader):\n",
    "  print(data.size())\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BtJBOtez11wy",
   "metadata": {
    "id": "BtJBOtez11wy"
   },
   "source": [
    "## 2. A parameterizeable network with snnTorch\n",
    "\n",
    "With this MNIST dataset, some things always remain the same. The input image size, and the fact that we want to detect one of 10 pixels. Those are hardwired.\n",
    "\n",
    "The first layer's decay rate might be adjustable as a hyperparameter. There is one neuron for each pixel.\n",
    "\n",
    "The output layer is fixed to classify digits 0-9. There is one neuron for each digit.\n",
    "\n",
    "We also track the average spike activity across the network, so that we can caluclate how much spiking activity per digit was generated.\n",
    "\n",
    "The number of timesteps before we get the result is configurable.\n",
    "\n",
    "Beta is the decay rate, the amount that each neuron remembers from the previous timestep (0 - no memory, 1 - never forget). *beta1* is the decay rate for the first layer, and it is a model parameter. All the other neurons learn their own beta during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JM2thnrc10rD",
   "metadata": {
    "id": "JM2thnrc10rD"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_steps, num_hidden_neurons=299, num_hidden_layers=1, beta1=0.9):\n",
    "        super().__init__()\n",
    "        assert 0 <= beta1 <= 1, \"Beta1 must be between 0 and 1\"\n",
    "        assert num_hidden_layers >= 0, \"Number of hidden layers must be non-negative\"\n",
    "\n",
    "        num_inputs = 28 * 28 # image is 28x28 pixels\n",
    "        num_outputs = 10 # we want to get digits 0-9\n",
    "        self.num_steps = num_steps\n",
    "        self.num_hidden_neurons = num_hidden_neurons\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # Initialize layers\n",
    "        self.layers = []\n",
    "        for n in range(num_hidden_layers + 1):\n",
    "            layer = {}\n",
    "            if n == 0:\n",
    "                # First layer\n",
    "                layer['fc'] = nn.Linear(num_inputs, num_hidden_neurons)\n",
    "                layer['lif'] = snn.Leaky(beta=beta1)\n",
    "            elif n < num_hidden_layers:\n",
    "                # Inner layers\n",
    "                layer['fc'] = nn.Linear(num_hidden_neurons, num_hidden_neurons)\n",
    "                beta2 = torch.rand((num_hidden_neurons), dtype=torch.float)\n",
    "                layer['lif'] = snn.Leaky(beta=beta2, learn_beta=True)\n",
    "            else:\n",
    "                # Output layer\n",
    "                layer['fc'] = nn.Linear(num_hidden_neurons, num_outputs)\n",
    "                beta2 = torch.rand((num_outputs), dtype=torch.float)\n",
    "                layer['lif'] = snn.Leaky(beta=beta2, learn_beta=True)\n",
    "\n",
    "            # Add the layers to the internal representation\n",
    "            self.add_module(f'fc{n}', layer['fc'])\n",
    "            self.add_module(f'lif{n}', layer['lif'])\n",
    "\n",
    "            # Add the layers to our layer list.\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # Reset spike counter\n",
    "        self.reset_spikes()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass.\n",
    "\n",
    "        # Initialize all the neurons in all layers\n",
    "        for layer in self.layers:\n",
    "            layer['mem'] = layer['lif'].init_leaky()\n",
    "\n",
    "        spk_rec, mem_rec = [], []\n",
    "\n",
    "        # process each timestep\n",
    "        for step in range(self.num_steps):\n",
    "            cur = x.flatten(1)\n",
    "\n",
    "            # process each layer\n",
    "            for index, layer in enumerate(self.layers):\n",
    "                # process the data\n",
    "                cur, layer['mem'] = layer['lif'](layer['fc'](cur), layer['mem'])\n",
    "\n",
    "                # update the total spike count\n",
    "                self.total_spike_count += cur.sum().item()\n",
    "            # update the spike records\n",
    "            spk_rec.append(cur)\n",
    "            mem_rec.append(self.layers[-1]['mem'])\n",
    "\n",
    "        self.forward_count += 1 # so we can normalize the spike_count later\n",
    "        return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "    def get_spikes_per_digit(self):\n",
    "        # Returns average number of spikes per forward pass\n",
    "        return self.total_spike_count/self.forward_count\n",
    "\n",
    "    def reset_spikes(self):\n",
    "        # Reset all the spike counting information\n",
    "        self.total_spike_count = 0 # How many spikes have been generated, in all layers\n",
    "        self.forward_count = 0 # How many forward passes have been made, altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48_7sIT86iUJ",
   "metadata": {
    "id": "48_7sIT86iUJ"
   },
   "source": [
    "## 3. The hyperparameter trainer\n",
    "\n",
    "The trainer class is here to define how the training takes place, given a network and a few training hyperparameters. It makes the objective below a bit more readable.\n",
    "\n",
    "It's a bit of a challange because all sorts of networks are evaluated here, deep ones, wide ones, many neurons, few neurons, and they all improve their loss and accuracy at different speeds. That's why this class has an automatic early stopping feature. Early stopping stops the training when there the loss has not significantly improved in the last *patience=300* batches. That avoids serious overtraing.\n",
    "\n",
    "We adjust the definition of significant improvement based on the number of layers. Bcause deep networks take longer to train and the improvements are on average correspondingly smaller per batch this seems about right:\n",
    "\n",
    "$$significane_{actual} = \\frac{significance_{base}}{layers^3}$$\n",
    "\n",
    "That means if there's no change for single layer networks (5% improvement in the last 300 steps). \n",
    "\n",
    "For a ten layer network, pretty much any tiny improvement is an improvement of significance. This method has been heuristically determined, and it seems to work quite well and gets most networks to a reasonably high training accuracy.\n",
    "\n",
    "The early stopping parameter is important, if it stops the training of deep networks too soon, it'll appear as if it is a bad network, but it was not trained well enough. If it overtrains a 1 layer network and it memorized the input, it'll wrongly appear as a bad net also. \n",
    "\n",
    "The early stop mechanism could also be optimized for (a hyper-hyperparameter), and can be as complicated as desired. For our spike optimizer, we keep this part constant because GPU time is still expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kGZf7Hr55psl",
   "metadata": {
    "id": "kGZf7Hr55psl"
   },
   "outputs": [],
   "source": [
    "class SNNTrainer:\n",
    "\n",
    "    def __init__(self, net, trial, num_epochs=30, num_steps=25, learning_rate=2e-3, patience = 300, sig_improvement = 0.05):\n",
    "        self.net = net.to(device)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_steps = num_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trial = trial\n",
    "        self.patience = patience\n",
    "        \n",
    "        # Calculate what we mean by significant improvement\n",
    "        self.sig_improvement = sig_improvement/(net.num_hidden_layers**3)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(),\n",
    "                                          lr=learning_rate,\n",
    "                                          betas=(0.9, 0.999))\n",
    "\n",
    "        self.loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "        self.loss_hist = []\n",
    "        self.acc_hist = []\n",
    "        self.epochs = 0\n",
    "        self.batches = 0\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        acc = 0\n",
    "        best_loss = float(\"inf\")\n",
    "        loss_counter = 0\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i, (data, targets) in enumerate(train_loader):\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                self.net.train()\n",
    "                spk_rec, _ = self.net(data)\n",
    "                loss_val = self.loss_fn(spk_rec, targets)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss_val.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                current_loss = loss_val.item()\n",
    "                self.loss_hist.append(current_loss)\n",
    "\n",
    "                # Update display every few iterations.\n",
    "                if i % 100 == 0 and i != 0:\n",
    "                    acc = SF.accuracy_rate(spk_rec, targets)\n",
    "                    self.acc_hist.append(acc)\n",
    "                    logger.info(f\"Trial {self.trial.number}: Training: Epoch {epoch}, Batch {i} \"+\n",
    "                                 f\"Loss: {loss_val.item():.4f} (best:{best_loss:.4f} t-{loss_counter}) \"+ \n",
    "                                 f\"Accuracy: {acc * 100:.2f}%\")\n",
    "                \n",
    "                # rudimentary early stop:\n",
    "                # After the first epoch, if there is no improvement, call it a day\n",
    "\n",
    "                if current_loss < (1-self.sig_improvement)*best_loss: # an improvement!\n",
    "                    best_loss = current_loss\n",
    "                    loss_counter = 0\n",
    "                else: # No improvement\n",
    "                    loss_counter += 1\n",
    "                    \n",
    "                self.batches += 1\n",
    "                \n",
    "                if epoch > 0 and loss_counter > self.patience:\n",
    "                    logger.info(\"Early stopping.\")\n",
    "                    return\n",
    "                \n",
    "            self.epochs += 1\n",
    "\n",
    "    def get_accuracy(self, test_loader):\n",
    "        # Get the normal test accuracy for the dataset provided.\n",
    "        total_acc = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            self.net.eval()\n",
    "            for data, targets in test_loader:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "                spk_rec, _ = self.net(data)\n",
    "\n",
    "                acc = SF.accuracy_rate(spk_rec, targets)\n",
    "                total_acc += acc * data.size(0)\n",
    "                total += data.size(0)\n",
    "        return total_acc / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd81db",
   "metadata": {
    "id": "97dd81db"
   },
   "source": [
    "## 4. The Objective\n",
    "\n",
    "We direct Optuna to optimize specific parameters within carefully chosen ranges, ensuring they are neither too broad nor too narrow. This balance is crucial, especially for parameters like the number of steps and epochs, as overly high values can significantly increase training time. Even these hyper-hyperparameters have to be chosen in some way.\n",
    "\n",
    "Because we want to explore many parameters, we are a bit generous here - the range can be somewhat on the large side for each of them. Before we do anything further, we then prune the ones that will contain too much information and pose a risk of overtraining. That, to the delight of the developer, generally conicides with ones that are very large and take a very long time to run.\n",
    "\n",
    "Then, it runs the training.\n",
    "\n",
    "After that, accuracy is gauged using a separate validation dataset. This contains data that this network has never seen - so we can see how well the network can generalize from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f04986",
   "metadata": {
    "id": "76ed5b58"
   },
   "outputs": [],
   "source": [
    "def optuna_objective(trial, train_loader, test_loader):\n",
    "    # Suggest hyperparameters, set the approximate range where we want to optimze\n",
    "    num_steps = trial.suggest_int('Timesteps', 10, 50)\n",
    "    num_hidden_layers = trial.suggest_int('Hidden Layers', 1, 10)\n",
    "    num_hidden_neurons = trial.suggest_int('Neurons per Hidden Layer', 5, 300)\n",
    "\n",
    "    learning_rate = trial.suggest_float('Learning Rate', 1e-4, 1e-2, log=True)\n",
    "    first_layer_beta = trial.suggest_float('First Layer β', 0.5, 1)\n",
    "\n",
    "    logger.info(f\"Trial {trial.number}: Training: Layers={num_hidden_layers} \"+\n",
    "          f\"Neurons={num_hidden_neurons} Steps={num_steps} l1Beta={first_layer_beta:2f}\")\n",
    "    \n",
    "    # Skip large networks with many steps, they take too long to train\n",
    "    # This cuts off a large corner of the parameter space - and the runtime\n",
    "    if num_hidden_layers*num_hidden_neurons*num_steps > 300*15:\n",
    "        raise TrialPruned(\"Too computationally intensive.\")\n",
    "\n",
    "    logger.info(f\"Trial {trial.number}: Running\")\n",
    "\n",
    "    net = Net(num_steps, num_hidden_neurons, num_hidden_layers, first_layer_beta)\n",
    "    \n",
    "    # Run the training!\n",
    "    trainer = SNNTrainer(net, trial, num_steps=num_steps, learning_rate=learning_rate)\n",
    "    training_start_time = time.time()\n",
    "    trainer.train(train_loader)\n",
    "    \n",
    "    # Training info - so we can plot it later\n",
    "    trial.set_user_attr(\"Training Time [s]\", time.time() - training_start_time)\n",
    "    trial.set_user_attr(\"Epochs\", trainer.epochs)\n",
    "    trial.set_user_attr(\"Batches\", trainer.batches)\n",
    "    \n",
    "    logger.info(f\"Trial {trial.number}: Run on validation set\")\n",
    "    net.reset_spikes() # Only consider spikes/digit after training is complete\n",
    "    validation_accuracy = trainer.get_accuracy(validation_loader)\n",
    "\n",
    "    # The thing we really want to optimize for!\n",
    "    spikes_per_digit = net.get_spikes_per_digit()\n",
    "\n",
    "    # Define the objective to maximize test accuracy and minimize spike count\n",
    "    return               validation_accuracy,   spikes_per_digit\n",
    "\n",
    "# The objectives have a printable name and direction\n",
    "# Optuna keeps track of the objectives returned as an ordered array, \n",
    "# so we do, too, all here in one place.\n",
    "objective_names      = [\"Validation Accuracy\", \"Spikes per Digit\"]\n",
    "objective_directions = [\"maximize\",            \"minimize\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d57e8a",
   "metadata": {
    "id": "93d57e8a"
   },
   "source": [
    "## 5. The study\n",
    "\n",
    "Now we can run the things we just defined and see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a093159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a093159",
    "outputId": "597f5a27-37b6-4925-d15a-ca1ec3d43635"
   },
   "outputs": [],
   "source": [
    "# Define the Optuna study\n",
    "# maximize accuracy\n",
    "# minimize spikes\n",
    "study = optuna.create_study(study_name=\"Minimize spikes, maximize accuracy\",\n",
    "                            directions=objective_directions)\n",
    "\n",
    "completed_trials = 0 # Nothing has been done yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to figure out how many trials have successfully completed\n",
    "def completed_trials(study):\n",
    "    # Counts the completed, successful trials\n",
    "    return sum(1 for trial in study.trials if trial.state == TrialState.COMPLETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced976b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "30f12ae2",
    "outputId": "f1c84acf-c675-4ed8-9e76-07cc08554ec2"
   },
   "outputs": [],
   "source": [
    "# Need at least 3 for the plots below\n",
    "additional_trials = 50\n",
    "\n",
    "# Bookkeeping\n",
    "start_time = time.time()\n",
    "start_trials = completed_trials(study)\n",
    "target_trials = start_trials + additional_trials\n",
    "logger.info(f\"Running on device={device}.\")\n",
    "logger.info(f\"{start_trials} completed. Running {additional_trials} more to have {target_trials} in total.\")\n",
    "\n",
    "while completed_trials(study) < target_trials:\n",
    "    # Run trials one at a time so we can stop the code block and keep whatever has been learned\n",
    "    study.optimize( lambda trial:\n",
    "                    optuna_objective(trial, train_loader, test_loader),\n",
    "                    n_trials=additional_trials)\n",
    "    \n",
    "    # Bookkeeping and message generation\n",
    "    elapsed = time.time() - start_time\n",
    "    total_completed = completed_trials(study)\n",
    "    completed = total_completed - start_trials\n",
    "    remaining_trials = target_trials - completed - start_trials\n",
    "    logger.info(f\"#### Remaining trials {remaining_trials} ####\")\n",
    "    if completed > 0:\n",
    "        rate = elapsed/(completed)\n",
    "        remaining_time = (target_trials - completed)*rate\n",
    "        logger.info(f\"Completed {total_completed}/{target_trials} studies at {rate/60:.1f}min/trial\")\n",
    "        if total_completed < target_trials:\n",
    "            logger.info(f\"Remaining time: {remaining_time/60:.1f} minutes to do {remaining_trials} trials.\")\n",
    "        \n",
    "logger.info(f\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437985d",
   "metadata": {
    "id": "b437985d"
   },
   "source": [
    "## 6. Ponder the Results\n",
    "\n",
    "Now it's time to actually look at the parameters and think about them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optunacy plotter\n",
    "see = OPlot(study, objective_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd8c8e",
   "metadata": {},
   "source": [
    "## 6.1 Cause and Effect\n",
    "\n",
    "We can look at the importance of hyperparameters on outcome metrics, and see what impact a change in hyperparameter input has on an output.\n",
    "\n",
    "I've already forgotten what we collected; let's see a list. That also allows easy copying of the strings because we'll make lots of plots now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3b619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "see.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb816d68",
   "metadata": {},
   "source": [
    "I am curious about deep networks with many hidden layers and if they are effective here. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "see.plot(\"Spikes per Digit\", \"Validation Accuracy\", \"Hidden Layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa50559b",
   "metadata": {},
   "source": [
    "In this plot, each dot is a Network, and the color indicates the hidden networks in a given area. \n",
    "Spikes per Digit is roughly proportiona to power consumption, and Validation Accuracy is a measure of how well the network works. So we want to be in the top left corner.\n",
    "But we can already see: The top left corner is dominated by one-layered networks. So my hypothesis was not right, deep networks make lots of spikes.\n",
    "\n",
    "It's a bit chaotic, and we absolutely don't care about accuracies below 60%. So let's zoom in a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a913f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "see.plot(\"Spikes per Digit\", \"Validation Accuracy\", \"Hidden Layers\", y_range=(0.60, 1), z_clip=(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a296234",
   "metadata": {},
   "source": [
    "Deeper networks are defnintely to the right. \n",
    "\n",
    "I'd guess that network size and spike rate are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592785b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "see.plot(\"Neurons per Hidden Layer\", \"Hidden Layers\", \"Validation Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c8b14",
   "metadata": {},
   "source": [
    "First, note that there are no datapoints in the top right part of the graph. That's because we prune these - lots of deep layers are very computationally expensive.\n",
    "\n",
    "In any case, the graph is not very informative, we mostly care about accuracies that are at the very least 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e49bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "see.plot(\"Neurons per Hidden Layer\", \"Hidden Layers\", \"Validation Accuracy\", z_clip=(.8,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f2baf",
   "metadata": {},
   "source": [
    "That's nice, it seems we need about 100-200 Neurons (if you mouse over a point you can see the data) on one or two layers, or more on 3 layers. Also, large networks seem to be not very accurate. Also, networks with very few neurons (in the bottom left corner) are not accurate. \n",
    "\n",
    "Let's look at the spike rate on that same picture. I'll clip it to see the interesting parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "see.plot(\"Neurons per Hidden Layer\", \"Hidden Layers\", \"Spikes per Digit\", z_clip=(20000,80000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d1429",
   "metadata": {},
   "source": [
    "From this it's clear that large networks are not power efficient.\n",
    "\n",
    "What about the other parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc4bf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "see.plot(\"First Layer β\", \"Validation Accuracy\", \"Spikes per Digit\", z_clip=(30000, 80000), y_range=(0.8,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafd5f0",
   "metadata": {},
   "source": [
    "That does not look particularly helpful. It seems like all values for β can provide high accuracy results, some even with low spike counts. It appears that there are more low spike count nets with high accuracy where β is close to 1, so maybe β should be greater than 0.95.\n",
    "\n",
    "What about timesteps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feceaad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "see.plot(\"Timesteps\", \"Validation Accuracy\", \"Spikes per Digit\", z_clip=(30000, 80000), y_range=(0.8,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a8ac5",
   "metadata": {},
   "source": [
    "As we can expect, the longer it runs, the more timesteps we get. It seems that the optimum numer of timesteps is around 15-20. \n",
    "\n",
    "What about Learning Rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff675c95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "see.plot(\"Learning Rate\", \"Validation Accuracy\", \"Spikes per Digit\", z_clip=(30000, 80000), y_range=(0.8,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff37c8c",
   "metadata": {},
   "source": [
    "Here, it seems like most of the results are in the top left corner. There's an area in the right top corner that is maybe underexplored. That's becuase the learning rate was run with a log distribution:\n",
    "\n",
    "```    learning_rate = trial.suggest_float('Learning Rate', 1e-4, 1e-2, log=True)```\n",
    "\n",
    "Maybe in the next run, take that off, and explore the top right corner also!\n",
    "\n",
    "Optuna has some more built in [plotting features](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html), for example, a way to plot the importance of a parameter for a particular optimization target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3612ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "bc3612ab",
    "outputId": "6b13faf7-ae8f-4120-82d7-238dab12579f"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study,\n",
    "                                  target=lambda t: t.values[0],\n",
    "                                  target_name = \"Validation accuracy\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4ed16",
   "metadata": {},
   "source": [
    "This plot means that the hyperparameter with the longest bar has the highest impact on accuracy. It does not say wether that number needs to be large or small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1909622",
   "metadata": {},
   "source": [
    "### 6.1 Summary\n",
    "\n",
    "From looking at the data, we've found that the optimal network is likely around\n",
    "\n",
    "- 1-2 layer deep\n",
    "- 100-200 total neurons\n",
    "- 15-20 timesteps long\n",
    "- at least 0.9 for the first layer's β\n",
    "\n",
    "This drastically reduces our searchspace, and we can re-run the optimizer with a focus in that area."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c8b87b4648a8d1ba1118329c37c7c28a2ff48490805f0e62ea19d4b1b49e5656"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
