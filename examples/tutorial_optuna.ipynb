{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "47d5313e-c29d-4581-a9c7-a45122337069",
      "metadata": {
        "id": "47d5313e-c29d-4581-a9c7-a45122337069"
      },
      "source": [
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/snntorch_alpha_w.png?raw=true' width=\"300\">](https://github.com/jeshraghian/snntorch/)\n",
        "\n",
        "# Discover SNN Hyperparameters with Optuna\n",
        "### Tutorial written by Reto Stamm\n",
        "<a href=\"https://colab.research.google.com/github/jeshraghian/snntorch/blob/master/examples/tutorial_optuna.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "[<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub-Mark-Light-120px-plus.png?raw=true' width=\"28\">](https://github.com/jeshraghian/snntorch/) [<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/GitHub_Logo_White.png?raw=true' width=\"80\">](https://github.com/jeshraghian/snntorch/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oll2NNFeG1NG",
      "metadata": {
        "id": "oll2NNFeG1NG"
      },
      "source": [
        "*This tutorial demonstrates optimizing Spiking Neural Network hyperparameters with Optuna, blending advanced neural modeling and hyperparameter tuning. In this example, we minimize power consumption by adjusting hyperparameters.*\n",
        "\n",
        "**[Optuna](https://optuna.org)** is an efficient, open-source hyperparameter optimization framework. It helps automatically figure out the best settings for machine learning models.\n",
        "\n",
        "In this tutorial, we will make the reasonable assumption that the more spikes in a network, the more power is consumed. We want to **minimize power consumption**, so we adjust the model's **hyperparameters**, including the shape of the network, the number of time-steps, and the number of epochs it is trained for."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a comprehensive overview on how SNNs work, and what is going on under the hood, [then you might be interested in the snnTorch tutorial series available here.](https://snntorch.readthedocs.io/en/latest/tutorials/index.html)\n",
        "The snnTorch tutorial series is based on the following paper. If you find these resources or code useful in your work, please consider citing the following source:\n",
        "\n",
        "> <cite> [Jason K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D. Lu. \"Training Spiking Neural Networks Using Lessons From Deep Learning\". Proceedings of the IEEE, 111(9) September 2023.](https://ieeexplore.ieee.org/abstract/document/10242251) </cite>"
      ],
      "metadata": {
        "id": "fbZR8GfbsUAM"
      },
      "id": "fbZR8GfbsUAM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDnIEHOKB8LD",
      "metadata": {
        "id": "hDnIEHOKB8LD",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install optuna snntorch optunacy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WL487gZW1Agy",
      "metadata": {
        "id": "WL487gZW1Agy"
      },
      "outputs": [],
      "source": [
        "# Import all the libraries\n",
        "import copy\n",
        "import logging\n",
        "import random\n",
        "import numbers\n",
        "import sys\n",
        "import time # To see how long each iteration takes\n",
        "import multiprocessing # To check how many cores we have\n",
        "\n",
        "import optuna # the optimizer\n",
        "from optuna.exceptions import TrialPruned # To abort, or prune inefficient parameter sets\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "# Basic torch tools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Image processing tools\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Extra plotting tools\n",
        "from optunacy.oplot import OPlot\n",
        "import scipy as scipy\n",
        "\n",
        "# Spiking Neural Networks!\n",
        "import snntorch as snn\n",
        "import snntorch.functional as SF\n",
        "# from snntorch import utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EYf13Gtx1OCj",
      "metadata": {
        "id": "EYf13Gtx1OCj"
      },
      "source": [
        "## 1. The MNIST Dataset\n",
        "### 1.1 Dataloading\n",
        "Define variables for dataloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eo4T5MC21hgD",
      "metadata": {
        "id": "eo4T5MC21hgD"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "data_path='/tmp/data/mnist'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "myFKqNx11qYS",
      "metadata": {
        "id": "myFKqNx11qYS"
      },
      "source": [
        "Load the dataset. This is mostly just boilerplate. Note that we are taking a subset of the dataset.\n",
        "\n",
        "We also do not use the test set for tuning hyperparameters, or we would be leaking test time data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3GdglZjK04cb",
      "metadata": {
        "id": "3GdglZjK04cb"
      },
      "outputs": [],
      "source": [
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "# The MNIST dataset contains black and white images (28x28) of digits from 0-9\n",
        "# 60,000 training images\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True,\n",
        "                             transform=transform)\n",
        "\n",
        "# Split out a validation subset\n",
        "total_size = len(mnist_train)\n",
        "val_size = int(total_size * 0.08)  # 8% for validation\n",
        "train_size = total_size - val_size  # Remaining for training\n",
        "\n",
        "# Split the dataset, the same way every time\n",
        "mnist_val = Subset(mnist_train, range(train_size, total_size))\n",
        "mnist_train = Subset(mnist_train, range(0, train_size))\n",
        "\n",
        "# 10,000 test images\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True,\n",
        "                            transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(mnist_val, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BtJBOtez11wy",
      "metadata": {
        "id": "BtJBOtez11wy"
      },
      "source": [
        "## 2. A parameterizeable network with snnTorch\n",
        "\n",
        "With this MNIST dataset, some things always remain the same:\n",
        "\n",
        "* The input image size, and the fact that we want to detect one of 10 pixels. Those are hardwired.\n",
        "\n",
        "* The output layer is fixed to classify digits 0-9. There is one neuron for each digit.\n",
        "\n",
        "* The first layer's decay rate *beta1* can be learnable or adjustable as a hyperparameter.\n",
        "\n",
        "* The number of timesteps before we get the result is configurable.\n",
        "\n",
        "We also track the average spike activity across the network, so that we can calculate how much spiking activity per digit was generated."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model definition below constructs the layers in a loop to ensure they are parameterizable."
      ],
      "metadata": {
        "id": "Von0U1CyxPJo"
      },
      "id": "Von0U1CyxPJo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JM2thnrc10rD",
      "metadata": {
        "id": "JM2thnrc10rD"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, num_steps, num_hidden_neurons=299, num_hidden_layers=1, beta1=0.9):\n",
        "        super().__init__()\n",
        "        assert 0 <= beta1 <= 1, \"Beta1 must be between 0 and 1\"\n",
        "        assert num_hidden_layers >= 0, \"Number of hidden layers must be non-negative\"\n",
        "\n",
        "        num_inputs = 28 * 28 # image is 28x28 pixels\n",
        "        num_outputs = 10 # we want to get digits 0-9\n",
        "        self.num_steps = num_steps\n",
        "        self.num_hidden_neurons = num_hidden_neurons\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "\n",
        "        # Initialize layers\n",
        "        self.layers = []\n",
        "        for n in range(num_hidden_layers + 1):\n",
        "            layer = {}\n",
        "            if n == 0:\n",
        "                # First layer\n",
        "                layer['fc'] = nn.Linear(num_inputs, num_hidden_neurons)\n",
        "                layer['lif'] = snn.Leaky(beta=beta1)\n",
        "            elif n < num_hidden_layers:\n",
        "                # Inner layers\n",
        "                layer['fc'] = nn.Linear(num_hidden_neurons, num_hidden_neurons)\n",
        "                beta2 = torch.rand((num_hidden_neurons), dtype=torch.float)\n",
        "                layer['lif'] = snn.Leaky(beta=beta2, learn_beta=True)\n",
        "            else:\n",
        "                # Output layer\n",
        "                layer['fc'] = nn.Linear(num_hidden_neurons, num_outputs)\n",
        "                beta2 = torch.rand((num_outputs), dtype=torch.float)\n",
        "                layer['lif'] = snn.Leaky(beta=beta2, learn_beta=True)\n",
        "\n",
        "            # Add the layers to the internal representation\n",
        "            self.add_module(f'fc{n}', layer['fc'])\n",
        "            self.add_module(f'lif{n}', layer['lif'])\n",
        "\n",
        "            # Add the layers to our layer list.\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        # Reset spike counter\n",
        "        self.reset_spikes()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The forward pass.\n",
        "\n",
        "        # Initialize all the neurons in all layers\n",
        "        for layer in self.layers:\n",
        "            layer['mem'] = layer['lif'].init_leaky()\n",
        "\n",
        "        spk_rec, mem_rec = [], []\n",
        "\n",
        "        # process each timestep\n",
        "        for step in range(self.num_steps):\n",
        "            cur = x.flatten(1)\n",
        "\n",
        "            # process each layer\n",
        "            for index, layer in enumerate(self.layers):\n",
        "                # process the data\n",
        "                cur, layer['mem'] = layer['lif'](layer['fc'](cur), layer['mem'])\n",
        "\n",
        "                # update the total spike count\n",
        "                self.total_spike_count += cur.sum().item()\n",
        "            # update the spike records\n",
        "            spk_rec.append(cur)\n",
        "            mem_rec.append(self.layers[-1]['mem'])\n",
        "\n",
        "        self.forward_count += 1 # so we can normalize the spike_count later\n",
        "        return torch.stack(spk_rec), torch.stack(mem_rec)\n",
        "\n",
        "    def get_spikes_per_digit(self):\n",
        "        # Returns average number of spikes per forward pass\n",
        "        return self.total_spike_count/self.forward_count\n",
        "\n",
        "    def reset_spikes(self):\n",
        "        # Reset all the spike counting information\n",
        "        self.total_spike_count = 0 # How many spikes have been generated, in all layers\n",
        "        self.forward_count = 0 # How many forward passes have been made, altogether"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48_7sIT86iUJ",
      "metadata": {
        "id": "48_7sIT86iUJ"
      },
      "source": [
        "## 3. The hyperparameter trainer\n",
        "\n",
        "The trainer class is here to define how the training takes place, given a network and a few training hyperparameters. It makes the objective below a bit more readable.\n",
        "\n",
        "This class includes an automatic early stopping feature. Early stopping completes the training loop when there the loss has not significantly improved in the last *patience=300* batches.\n",
        "\n",
        "What does 'significant improvement' mean? Whatever you want. In our case, we make it a function of the number of layers. Deep networks take longer to train and the improvements are, on average, smaller per batch, the formula below is a rough way to account for this:\n",
        "\n",
        "$$significance_{actual} = \\frac{significance_{base}}{layers^3}$$\n",
        "\n",
        "By default, e.g., for a single layer network, the training run will terminate if there isn't anything more than a 5% improvement in the last 300 training steps.\n",
        "\n",
        "For a ten layer network, pretty much any tiny improvement is an improvement of significance. This method has been heuristically determined, and seems to work quite well."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logs must be in sync with Optunas output\n",
        "logger = logging.getLogger('optuna')"
      ],
      "metadata": {
        "id": "x3a6P89Wvg_i"
      },
      "id": "x3a6P89Wvg_i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "B5FKVGmhvhuL"
      },
      "id": "B5FKVGmhvhuL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kGZf7Hr55psl",
      "metadata": {
        "id": "kGZf7Hr55psl"
      },
      "outputs": [],
      "source": [
        "class SNNTrainer:\n",
        "\n",
        "    def __init__(self, net, trial, num_epochs=30, num_steps=25,\n",
        "                 learning_rate=2e-3, patience = 300, sig_improvement = 0.05):\n",
        "\n",
        "        self.net = net.to(device)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.num_steps = num_steps\n",
        "        self.learning_rate = learning_rate\n",
        "        self.trial = trial\n",
        "        self.patience = patience\n",
        "\n",
        "        # Calculate what we mean by significant improvement\n",
        "        self.sig_improvement = sig_improvement/(net.num_hidden_layers**3)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(),\n",
        "                                          lr=learning_rate,\n",
        "                                          betas=(0.9, 0.999))\n",
        "\n",
        "        self.loss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
        "        self.loss_hist = []\n",
        "        self.acc_hist = []\n",
        "        self.epochs = 0\n",
        "        self.batches = 0\n",
        "\n",
        "    def train(self, train_loader):\n",
        "        acc = 0\n",
        "        best_loss = float(\"inf\")\n",
        "        loss_counter = 0\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            for i, (data, targets) in enumerate(train_loader):\n",
        "                data = data.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                self.net.train()\n",
        "                spk_rec, _ = self.net(data)\n",
        "                loss_val = self.loss_fn(spk_rec, targets)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss_val.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                current_loss = loss_val.item()\n",
        "                self.loss_hist.append(current_loss)\n",
        "\n",
        "                # Update display every few iterations.\n",
        "                if i % 100 == 0 and i != 0:\n",
        "                    acc = SF.accuracy_rate(spk_rec, targets)\n",
        "                    self.acc_hist.append(acc)\n",
        "                    logger.info(f\"Trial {self.trial.number}: Training: Epoch {epoch}, Batch {i} \"+\n",
        "                                 f\"Loss: {loss_val.item():.4f} (best:{best_loss:.4f} t-{loss_counter}) \"+\n",
        "                                 f\"Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "                # rudimentary early stop:\n",
        "                # After the first epoch, if there is no improvement, call it a day\n",
        "                if current_loss < (1-self.sig_improvement)*best_loss: # an improvement!\n",
        "                    best_loss = current_loss\n",
        "                    loss_counter = 0\n",
        "                else: # No improvement\n",
        "                    loss_counter += 1\n",
        "\n",
        "                self.batches += 1\n",
        "\n",
        "                if epoch > 0 and loss_counter > self.patience:\n",
        "                    logger.info(\"Early stopping.\")\n",
        "                    return\n",
        "\n",
        "            self.epochs += 1\n",
        "\n",
        "    def get_accuracy(self, test_loader):\n",
        "        # Get the normal test accuracy for the dataset provided.\n",
        "        total_acc = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            self.net.eval()\n",
        "            for data, targets in test_loader:\n",
        "                data = data.to(device)\n",
        "                targets = targets.to(device)\n",
        "                spk_rec, _ = self.net(data)\n",
        "\n",
        "                acc = SF.accuracy_rate(spk_rec, targets)\n",
        "                total_acc += acc * data.size(0)\n",
        "                total += data.size(0)\n",
        "        return total_acc / total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97dd81db",
      "metadata": {
        "id": "97dd81db"
      },
      "source": [
        "## 4. The Objective\n",
        "\n",
        "We have two targets:\n",
        "\n",
        "* maximize accuracy\n",
        "* minimize spikes\n",
        "\n",
        "We direct Optuna to optimize specific parameters within carefully chosen ranges, ensuring they are neither too broad nor too narrow. This balance is crucial, especially for parameters like the number of steps and epochs, as overly high values can significantly increase training time. Even these hyper-hyperparameters have to be chosen in some way.\n",
        "\n",
        "We have included a termination policy below that kills training runs that are too expensive.\n",
        "\n",
        "Following the training run, accuracy is measuring using a separate validation dataset. This contains data that this network has never seen - so we can see how well the network can generalize from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f04986",
      "metadata": {
        "id": "a1f04986"
      },
      "outputs": [],
      "source": [
        "def optuna_objective(trial, train_loader, test_loader):\n",
        "    # Suggest hyperparameters, set the approximate range where we want to optimze\n",
        "    num_steps = trial.suggest_int('Timesteps', 10, 50)\n",
        "    num_hidden_layers = trial.suggest_int('Hidden Layers', 1, 10)\n",
        "    num_hidden_neurons = trial.suggest_int('Neurons per Hidden Layer', 5, 300)\n",
        "\n",
        "    learning_rate = trial.suggest_float('Learning Rate', 1e-4, 1e-2, log=True)\n",
        "    first_layer_beta = trial.suggest_float('First Layer β', 0.5, 1)\n",
        "\n",
        "    logger.info(f\"Trial {trial.number}: Training: Layers={num_hidden_layers} \"+\n",
        "          f\"Neurons={num_hidden_neurons} Steps={num_steps} l1Beta={first_layer_beta:2f}\")\n",
        "\n",
        "    # Skip large networks with many steps, they take too long to train\n",
        "    # This cuts off a large corner of the parameter space - and the runtime\n",
        "    if num_hidden_layers*num_hidden_neurons*num_steps > 300*15:\n",
        "        raise TrialPruned(\"Too computationally intensive.\")\n",
        "\n",
        "    logger.info(f\"Trial {trial.number}: Running\")\n",
        "\n",
        "    net = Net(num_steps, num_hidden_neurons, num_hidden_layers, first_layer_beta)\n",
        "\n",
        "    # Run the training!\n",
        "    trainer = SNNTrainer(net, trial, num_steps=num_steps, learning_rate=learning_rate)\n",
        "    training_start_time = time.time()\n",
        "    trainer.train(train_loader)\n",
        "\n",
        "    # Training info - so we can plot it later\n",
        "    trial.set_user_attr(\"Training Time [s]\", time.time() - training_start_time)\n",
        "    trial.set_user_attr(\"Epochs\", trainer.epochs)\n",
        "    trial.set_user_attr(\"Batches\", trainer.batches)\n",
        "\n",
        "    logger.info(f\"Trial {trial.number}: Run on validation set\")\n",
        "    net.reset_spikes() # Only consider spikes/digit after training is complete\n",
        "    validation_accuracy = trainer.get_accuracy(validation_loader)\n",
        "\n",
        "    # The thing we really want to optimize for!\n",
        "    spikes_per_digit = net.get_spikes_per_digit()\n",
        "\n",
        "    # Define the objective to maximize test accuracy and minimize spike count\n",
        "    return               validation_accuracy,   spikes_per_digit\n",
        "\n",
        "# The objectives have a printable name and direction\n",
        "# Optuna keeps track of the objectives returned as an ordered array,\n",
        "# so we do, too, all here in one place.\n",
        "objective_names      = [\"Validation Accuracy\", \"Spikes per Digit\"]\n",
        "objective_directions = [\"maximize\",            \"minimize\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d57e8a",
      "metadata": {
        "id": "93d57e8a"
      },
      "source": [
        "## 5. The study\n",
        "\n",
        "Now we can run the things we just defined and see the results! This will take a considerable amount of time. Reduce `additional_trials` if you'd like to speed things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a093159",
      "metadata": {
        "id": "5a093159"
      },
      "outputs": [],
      "source": [
        "# Define the Optuna study\n",
        "# maximize accuracy\n",
        "# minimize spikes\n",
        "study = optuna.create_study(study_name=\"Minimize spikes, maximize accuracy\",\n",
        "                            directions=objective_directions)\n",
        "\n",
        "completed_trials = 0 # Nothing has been done yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3081df11",
      "metadata": {
        "id": "3081df11"
      },
      "outputs": [],
      "source": [
        "# Helper to figure out how many trials have successfully completed\n",
        "def completed_trials(study):\n",
        "    # Counts the completed, successful trials\n",
        "    return sum(1 for trial in study.trials if trial.state == TrialState.COMPLETE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ced976b4",
      "metadata": {
        "id": "ced976b4"
      },
      "outputs": [],
      "source": [
        "# Need at least 3 for the plots below\n",
        "additional_trials = 50\n",
        "\n",
        "# Bookkeeping\n",
        "start_time = time.time()\n",
        "start_trials = completed_trials(study)\n",
        "target_trials = start_trials + additional_trials\n",
        "logger.info(f\"Running on device={device}.\")\n",
        "logger.info(f\"{start_trials} completed. Running {additional_trials} more to have {target_trials} in total.\")\n",
        "\n",
        "while completed_trials(study) < target_trials:\n",
        "    # Run trials one at a time so we can stop the code block and keep whatever has been learned\n",
        "    study.optimize( lambda trial:\n",
        "                    optuna_objective(trial, train_loader, test_loader),\n",
        "                    n_trials=additional_trials)\n",
        "\n",
        "    # Bookkeeping and message generation\n",
        "    elapsed = time.time() - start_time\n",
        "    total_completed = completed_trials(study)\n",
        "    completed = total_completed - start_trials\n",
        "    remaining_trials = target_trials - completed - start_trials\n",
        "    logger.info(f\"#### Remaining trials {remaining_trials} ####\")\n",
        "    if completed > 0:\n",
        "        rate = elapsed/(completed)\n",
        "        remaining_time = (target_trials - completed)*rate\n",
        "        logger.info(f\"Completed {total_completed}/{target_trials} studies at {rate/60:.1f}min/trial\")\n",
        "        if total_completed < target_trials:\n",
        "            logger.info(f\"Remaining time: {remaining_time/60:.1f} minutes to do {remaining_trials} trials.\")\n",
        "\n",
        "logger.info(f\"DONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b437985d",
      "metadata": {
        "id": "b437985d"
      },
      "source": [
        "## 6. Ponder the Results\n",
        "\n",
        "Now it's time to actually look at the parameters and think about them!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9af5e41",
      "metadata": {
        "id": "d9af5e41"
      },
      "outputs": [],
      "source": [
        "# Initialize the optunacy plotter\n",
        "see = OPlot(study, objective_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddfd8c8e",
      "metadata": {
        "id": "ddfd8c8e"
      },
      "source": [
        "## 6.1 Cause and Effect\n",
        "\n",
        "We can look at the importance of hyperparameters on outcome metrics, and see what impact a change in hyperparameter input has on an output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c3b619",
      "metadata": {
        "scrolled": true,
        "id": "89c3b619"
      },
      "outputs": [],
      "source": [
        "see.parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb816d68",
      "metadata": {
        "id": "fb816d68"
      },
      "source": [
        "I am curious about deep networks with many hidden layers and if they are effective here. Let's see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab4354f",
      "metadata": {
        "id": "dab4354f"
      },
      "outputs": [],
      "source": [
        "see.plot(\"Spikes per Digit\", \"Validation Accuracy\", \"Hidden Layers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa50559b",
      "metadata": {
        "id": "fa50559b"
      },
      "source": [
        "In this plot, each dot is a Network, and the color indicates the hidden networks in a given area.\n",
        "Spikes per Digit is roughly proportional to power consumption, and Validation Accuracy is a measure of how well the network works. So we want to be in the top left corner.\n",
        "But we can already see: The top left corner is dominated by one-layered networks. So my hypothesis was not right, deep networks make lots of spikes.\n",
        "\n",
        "It's a bit chaotic, and we absolutely don't care about accuracies below 60%. So let's zoom in a bit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713a913f",
      "metadata": {
        "scrolled": false,
        "id": "713a913f"
      },
      "outputs": [],
      "source": [
        "see.plot(\"Spikes per Digit\", \"Validation Accuracy\", \"Hidden Layers\", y_range=(0.60, 1), z_clip=(1,5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a296234",
      "metadata": {
        "id": "8a296234"
      },
      "source": [
        "Deeper networks are definitely to the right.\n",
        "\n",
        "I'd guess that network size and spike rate are correlated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b592785b",
      "metadata": {
        "scrolled": true,
        "id": "b592785b"
      },
      "outputs": [],
      "source": [
        "see.plot(\"Neurons per Hidden Layer\", \"Hidden Layers\", \"Validation Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a05c8b14",
      "metadata": {
        "id": "a05c8b14"
      },
      "source": [
        "First, note that there are no datapoints in the top right part of the graph. That's because we prune these - lots of deep layers are very computationally expensive.\n",
        "\n",
        "In any case, the graph is not very informative, we mostly care about accuracies that are at the very least 80%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8e49bf",
      "metadata": {
        "scrolled": false,
        "id": "4a8e49bf"
      },
      "outputs": [],
      "source": [
        "see.plot(\"Neurons per Hidden Layer\", \"Hidden Layers\", \"Validation Accuracy\", z_clip=(.8,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07f2baf",
      "metadata": {
        "id": "e07f2baf"
      },
      "source": [
        "That's nice, it seems we need about 100-200 Neurons (if you mouse over a point you can see the data) on one or two layers, or more on 3 layers. Also, large networks seem not to be very accurate. Also, networks with very few neurons (in the bottom left corner) are not accurate.\n",
        "\n",
        "Let's look at the spike rate on that same picture. I'll clip it to see the interesting parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ae11c9",
      "metadata": {
        "id": "74ae11c9"
      },
      "outputs": [],
      "source": [
        "see.plot(\"Neurons per Hidden Layer\", \"Hidden Layers\", \"Spikes per Digit\", z_clip=(20000,80000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "327d1429",
      "metadata": {
        "id": "327d1429"
      },
      "source": [
        "From this, it's clear that large networks are not power efficient.\n",
        "\n",
        "What about the other parameters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5bc4bf6",
      "metadata": {
        "scrolled": false,
        "id": "c5bc4bf6"
      },
      "outputs": [],
      "source": [
        "see.plot(\"First Layer β\", \"Validation Accuracy\", \"Spikes per Digit\", z_clip=(30000, 80000), y_range=(0.8,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cafd5f0",
      "metadata": {
        "id": "0cafd5f0"
      },
      "source": [
        "That does not look particularly helpful. It seems like all values for β can provide high accuracy results, some even with low spike counts. It appears that there are more low spike count nets with high accuracy where β is close to 1, so maybe β should be greater than 0.95.\n",
        "\n",
        "What about timesteps?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feceaad3",
      "metadata": {
        "id": "feceaad3"
      },
      "outputs": [],
      "source": [
        "see.plot(\"Timesteps\", \"Validation Accuracy\", \"Spikes per Digit\", z_clip=(30000, 80000), y_range=(0.8,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729a8ac5",
      "metadata": {
        "id": "729a8ac5"
      },
      "source": [
        "As we can expect, the longer it runs, the more timesteps we get. It seems that the optimum numer of timesteps is around 15-20.\n",
        "\n",
        "What about Learning Rate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff675c95",
      "metadata": {
        "scrolled": false,
        "id": "ff675c95"
      },
      "outputs": [],
      "source": [
        "see.plot(\"Learning Rate\", \"Validation Accuracy\", \"Spikes per Digit\", z_clip=(30000, 80000), y_range=(0.8,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff37c8c",
      "metadata": {
        "id": "9ff37c8c"
      },
      "source": [
        "Here, it seems like most of the results are in the top left corner. There's an area in the right top corner that is maybe underexplored. That's becuase the learning rate was run with a log distribution:\n",
        "\n",
        "```    learning_rate = trial.suggest_float('Learning Rate', 1e-4, 1e-2, log=True)```\n",
        "\n",
        "Maybe in the next run, take that off, and explore the top right corner also!\n",
        "\n",
        "Optuna has some more built in [plotting features](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html), for example, a way to plot the importance of a parameter for a particular optimization target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3612ab",
      "metadata": {
        "id": "bc3612ab"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_param_importances(study,\n",
        "                                  target=lambda t: t.values[0],\n",
        "                                  target_name = \"Validation accuracy\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f4ed16",
      "metadata": {
        "id": "90f4ed16"
      },
      "source": [
        "This plot means that the hyperparameter with the longest bar has the highest impact on accuracy. It does not say wether that number needs to be large or small."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1909622",
      "metadata": {
        "id": "f1909622"
      },
      "source": [
        "### 6.1 Summary\n",
        "\n",
        "From looking at the data, we've found that the optimal network is likely around\n",
        "\n",
        "- 1-2 layer deep\n",
        "- 100-200 total neurons\n",
        "- 15-20 timesteps long\n",
        "- at least 0.9 for the first layer's β\n",
        "\n",
        "This drastically reduces our searchspace, and we can re-run the optimizer with a focus in that area."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "c8b87b4648a8d1ba1118329c37c7c28a2ff48490805f0e62ea19d4b1b49e5656"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}